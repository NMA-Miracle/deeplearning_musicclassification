{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NMA-Miracle/deeplearning_musicclassification/blob/main/train_model_plus_eval_vanilla1_augment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "r20hiIsHrv-G"
      },
      "source": [
        "# Music classification and generation with spectrograms\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Beatrix Benko, Lina Teichmann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Ghm1piZLrv-I"
      },
      "source": [
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZusBgaLUrv-J"
      },
      "source": [
        "## This notebook\n",
        "This notebook loads the GTZAN dataset which includes audiofiles and spectrograms. You can use this dataset or find your own. The first part of the notebook is all about data visualization and show how to make spectrograms from audiofiles. The second part of the notebook includes a CNN that is trained on the spectrograms to predict music genre. Below we also provide links to tutorials and other resources if you want to try to do some of the harder project ideas. \n",
        "\n",
        "Have fun :) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Iu6LCcSKrv-K"
      },
      "source": [
        "## Acknowledgements\n",
        "This notebook was written by Beatrix Benkő and Lina Teichmann.\n",
        "\n",
        "**Useful code examples:** \n",
        "\n",
        "https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8\n",
        "\n",
        "[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)\n",
        "\n",
        "[https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py)\n",
        "\n",
        "https://github.com/kamalesh0406/Audio-Classification \n",
        "\n",
        "https://github.com/zcaceres/spec_augment\n",
        "\n",
        "https://musicinformationretrieval.com/ipython_audio.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3C9ea4NTrv-M"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b29WnN1u-Uy5",
        "outputId": "ab4f5237-297b-4dd5-820d-282d9756ed48"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_drive_path = '/content/drive/MyDrive/NMA/music.zip'\n",
        "%cd '/content/drive/MyDrive/NMA'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAV-Ty6E-5jP",
        "outputId": "38db6f80-0905-415f-c070-8de27046f4af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1WB812V4KRTgPyreWuW0usRMhTeRGD7zG/NMA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {},
        "id": "QFvunVdGrv-O",
        "outputId": "6952b283-686d-4eb9-cefa-b225e448cd34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/.shortcut-targets-by-id/1WB812V4KRTgPyreWuW0usRMhTeRGD7zG/NMA\n",
            "Installing collected packages: src\n",
            "  Attempting uninstall: src\n",
            "    Found existing installation: src 0.0.0\n",
            "    Can't uninstall 'src'. No files were found to uninstall.\n",
            "  Running setup.py develop for src\n",
            "Successfully installed src-0.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchaudio-augmentations in /usr/local/lib/python3.7/dist-packages (0.2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (1.12.0+cu113)\n",
            "Requirement already satisfied: wavaugment in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (0.2)\n",
            "Requirement already satisfied: julius in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (0.2.6)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch-pitch-shift in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchaudio-augmentations) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchaudio-augmentations) (4.1.1)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.7/dist-packages (from torch-pitch-shift->torchaudio-augmentations) (1.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from torch-pitch-shift->torchaudio-augmentations) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->torch-pitch-shift->torchaudio-augmentations) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch_audiomentations in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations) (1.12.0+cu113)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations) (1.2.2)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations) (0.2.6)\n",
            "Requirement already satisfied: torchaudio>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations) (0.12.0+cu113)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from torch_audiomentations) (0.8.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (1.0.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (1.1.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (0.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (21.3)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (2.1.9)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.0->torch_audiomentations) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.0->torch_audiomentations) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.6.0->torch_audiomentations) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.4.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa>=0.6.0->torch_audiomentations) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa>=0.6.0->torch_audiomentations) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.0->torch_audiomentations) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->torch_audiomentations) (4.1.1)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.7/dist-packages (from torch-pitch-shift>=1.2.2->torch_audiomentations) (1.3)\n"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get install -y ffmpeg --quiet\n",
        "!pip install librosa --quiet\n",
        "!pip install imageio --quiet\n",
        "!pip install imageio-ffmpeg --quiet\n",
        "# !pip install Ipython --upgrade\n",
        "!pip install -e .\n",
        "!pip install torchaudio-augmentations\n",
        "!pip install torch_audiomentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd './Data'\n",
        "# %pwd "
      ],
      "metadata": {
        "id": "9ZrYfTxh9DIA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipping\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/NMA/Data/genres_original.zip'\n",
        "# from zipfile import ZipFile\n",
        "# with ZipFile(file_path, 'r') as zipObj:\n",
        "#   # Extract all the contents of zip file in different directory\n",
        "#   zipObj.extractall()"
      ],
      "metadata": {
        "id": "oxjYULjM8hSf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fphYOM359d03",
        "outputId": "0f851efa-d3c4-42da-ea16-573cf9a99698"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1WB812V4KRTgPyreWuW0usRMhTeRGD7zG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {},
        "id": "C8YeKnowrv-P"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries.\n",
        "# %load_ext autoreload # live update of our functions\n",
        "# %autoreload 2\n",
        "\n",
        "#@title Imports\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import soundfile as sp\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import imageio\n",
        "import random, shutil\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "import librosa\n",
        "#import librosa.display\n",
        "import requests\n",
        "\n",
        "\n",
        "# from src.z_transform import ZTransformer\n",
        "# from src.conv_net import ConvolutionalMusicNet, train\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as Func\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from torch.utils import data\n",
        "import random, shutil\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio_augmentations import (\n",
        "    Compose,\n",
        "    Delay,\n",
        "    Gain,\n",
        "    HighLowPass,\n",
        "    Noise,\n",
        "    PitchShift,\n",
        "    PolarityInversion,\n",
        "    RandomApply,\n",
        "    RandomResizedCrop,\n",
        "    Reverb,\n",
        ")\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "id": "DAS1L9HT8VwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0438090e-9604-4506-a59a-a935c113ec7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {},
        "id": "DZ367bE9rv-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8df38f3-ae22-413e-8661-92e2a70e776b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is enabled in this notebook.\n"
          ]
        }
      ],
      "source": [
        "# @title Helper functions (run me)\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "      print(\"WARNING: For this notebook to perform best, \"\n",
        "          \"if possible, in the menu under `Runtime` -> \"\n",
        "          \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "      print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "\n",
        "#  Plotting function.\n",
        "\n",
        "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
        "  epochs = len(train_loss)\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
        "  ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_title('Epoch vs Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
        "  ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_title('Epoch vs Accuracy')\n",
        "  ax2.legend()\n",
        "  fig.set_size_inches(15.5, 5.5)\n",
        "  plt.show()\n",
        "\n",
        "device = set_device()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Custom class to split and save the dataset\n",
        "class Utils:\n",
        "  def __init__(self, data_dir):\n",
        "    self.data_dir =  data_dir\n",
        "    self.folder_names = [f'{data_dir}/train/', f'{data_dir}/test/', f'{data_dir}/val/']\n",
        "\n",
        "  def split_dataset(self)->None:\n",
        "    audio_dir = f\"{self.data_dir}/genres_original/\"\n",
        "    train_dir = self.folder_names[0]\n",
        "    test_dir = self.folder_names[1]\n",
        "    val_dir = self.folder_names[2]\n",
        "\n",
        "    for f in self.folder_names:\n",
        "      if os.path.exists(f):\n",
        "        shutil.rmtree(f)\n",
        "        os.mkdir(f)\n",
        "      else:\n",
        "        os.mkdir(f)\n",
        "\n",
        "    # Loop over all genres.\n",
        "\n",
        "    genres = list(os.listdir(audio_dir))\n",
        "    for g in genres:\n",
        "      # find all audios & split in train, test, and validation\n",
        "      src_file_paths= []\n",
        "      for im in glob.glob(os.path.join(audio_dir, f'{g}',\"*.wav\"), recursive=True):\n",
        "        src_file_paths.append(im)\n",
        "      random.shuffle(src_file_paths)\n",
        "      test_files = src_file_paths[0:10]\n",
        "      val_files = src_file_paths[10:20]\n",
        "      train_files = src_file_paths[20:]\n",
        "\n",
        "      #  make destination folders for train and test images\n",
        "      for f in self.folder_names:\n",
        "        if not os.path.exists(os.path.join(f + f\"{g}\")):\n",
        "          os.mkdir(os.path.join(f + f\"{g}\"))\n",
        "\n",
        "      # copy training and testing images over\n",
        "      for f in train_files:\n",
        "        shutil.copy(f, os.path.join(os.path.join(train_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "      for f in test_files:\n",
        "        shutil.copy(f, os.path.join(os.path.join(test_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "      for f in val_files:\n",
        "        shutil.copy(f, os.path.join(os.path.join(val_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "  \n",
        "  \n",
        "  def create_dataset(self,files_dir: str,out_file:str, dur=3)->None:\n",
        "    \"\"\"\n",
        "      @param: files_dir -- str, takes the directory path to the subfolders containing our music genres\n",
        "      @param: out_file -- str, takes the file path where the created n seconds audio and labels will be stored\n",
        "      @param: dur -- int, takes the select split duration \n",
        "\n",
        "      @return: None\n",
        "    \n",
        "    \"\"\"\n",
        "    CLASSES =  os.listdir(files_dir)\n",
        "    dur = dur\n",
        "    X = [] # n seconds samples audio list\n",
        "    labels = [] # label indices\n",
        "    for dir in os.listdir(files_dir):\n",
        "      sub_dir = Path(files_dir, dir)\n",
        "      for file in sub_dir.glob(\"*.wav\"):\n",
        "        if(file.name != 'jazz.00054.wav'):\n",
        "          y,sr = librosa.load(file, sr=None)\n",
        "          stepsize = int(sr * dur)\n",
        "          for idx in range(0, len(y),  stepsize):\n",
        "            x = y[idx :  idx + stepsize ]\n",
        "            if(len(x) >=  stepsize):\n",
        "              X.append(x)\n",
        "              labels.append(CLASSES.index(dir))\n",
        "    X =  np.array(X)\n",
        "    labels = np.array(labels)\n",
        "    np.savez(file=out_file,X =  X, y = labels,  C = CLASSES)\n",
        "    print(f\"A dataset of musics with {X.shape[0]} samples created successfully\")\n",
        "  \n",
        "  def load(self, filename):\n",
        "    if( filename.is_file()):\n",
        "      data =  np.load(filename, allow_pickle=True)\n",
        "      X =  data['X'] #n seconds audio samples\n",
        "      y =  data['y'] #int label indices \n",
        "      CLASSES  = data['C'] # List of CLASSES\n",
        "      return X,y,CLASSES\n",
        "    else:\n",
        "      return None,None,None"
      ],
      "metadata": {
        "id": "Bb6jQFyNzkpD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are going to test our class\n",
        "data_dir = './Data'\n",
        "utils =  Utils(data_dir)\n",
        "\n",
        "utils.folder_names\n",
        "\n",
        "train_dir = Path('/content/drive/MyDrive/NMA/Data/train')\n",
        "test_dir = Path('/content/drive/MyDrive/NMA/Data/test')\n",
        "val_dir = Path('/content/drive/MyDrive/NMA/Data/val')\n",
        "\n",
        "train_file = Path('/content/drive/MyDrive/NMA/Data/train.npz')\n",
        "test_file = Path('/content/drive/MyDrive/NMA/Data/test.npz')\n",
        "val_file = Path('/content/drive/MyDrive/NMA/Data/val.npz')\n",
        "\n",
        "# utils.split_dataset()"
      ],
      "metadata": {
        "id": "2xSPjGNG0YLa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.create_dataset(train_dir, out_file=train_file,dur=3)\n",
        "# utils.create_dataset(test_dir, out_file=test_file,dur=3)\n",
        "# utils.create_dataset(val_dir, out_file=val_file,dur=3)"
      ],
      "metadata": {
        "id": "gzGjW2s75vc6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir"
      ],
      "metadata": {
        "id": "xkwFGhqUAajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b46b4-731a-4937-935f-ba646f87773a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/NMA/Data/test')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loading and test\n",
        "\n",
        "# X,y,CLASSES = utils.load(train_file)\n",
        "# X.shape "
      ],
      "metadata": {
        "id": "Blx-dvrV_ECV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdaa203-f95f-4fce-f97f-3ebe0f6e44b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7994, 66150)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X,y,CLASSES = utils.load(test_file)\n",
        "# X.shape "
      ],
      "metadata": {
        "id": "dRWtsqCc__44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec40ef5d-34e9-4581-c4bb-fb6e45b847dc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(998, 66150)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X,y,CLASSES = utils.load(val_file)\n",
        "# X.shape "
      ],
      "metadata": {
        "id": "_CSnDCCCAL4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38218e0e-c0a4-4c1b-a685-3d03aa1d9684"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989, 66150)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ConvolutionalMusicNet()"
      ],
      "metadata": {
        "id": "lD97RqUHcY5T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1b85fef9-c3bd-4b6e-fc16-5e23d6cd30b8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a0a7ed58bb18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mConvolutionalMusicNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ConvolutionalMusicNet' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spectrograms_dir = \"Data/images_original/\"\n",
        "folder_names = ['Data/train/', 'Data/test/', 'Data/val/']\n",
        "train_dir = folder_names[0]\n",
        "test_dir = folder_names[1]\n",
        "val_dir = folder_names[2]\n"
      ],
      "metadata": {
        "id": "B08N4vQHXueL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify dataset class\n",
        "class MiracleDataset(Dataset):\n",
        "  def __init__(self, X:np.ndarray,y:np.ndarray,n_fft=1024,hop_length = 512,sr=22050, loader=None,win_length = None,transform=None, target_transform=None):\n",
        "    self.n_fft = n_fft\n",
        "    self.win_length = win_length\n",
        "    self.hop_length = hop_length\n",
        "    self.sr = sr\n",
        "    self.X =  X\n",
        "    self.labels =  y \n",
        "    self.transform =  transform \n",
        "    self.target_transform =  target_transform\n",
        "    self.loader = loader\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    audio =  torch.from_numpy(self.X[idx])\n",
        "    # audio.to(device)\n",
        "    #audio =  self.X[idx].to(device)\n",
        "    label =  self.labels[idx]\n",
        "    #label = torch.from_numpy(label)\n",
        "    #label.to(device)\n",
        "\n",
        "    if self.transform and self.loader == 'train':\n",
        "      self.orig_audio = audio\n",
        "      audio = self.transform(audio)\n",
        "      #Manual pitchshifting \n",
        "      p01 = torch.rand(1)\n",
        "      p02 = torch.rand(1)\n",
        "      if p01 <= 0.4:\n",
        "        audio = PitchShift(sample_rate=self.sr, n_samples=audio.shape[0], pitch_shift_min=4, pitch_shift_max=5)(audio.reshape(1,audio.shape[0]))\n",
        "        #print(f'Shape after Pitching: {audio.shape}')\n",
        "      # if p02 <= 0.3:\n",
        "      #   audio = Reverb(sample_rate=self.sr,reverberance_min=90,reverberance_max=91,room_size_min=90,room_size_max=91,)(audio)\n",
        "      #   print(f'Shape after Reverbing: {audio.shape}')\n",
        "    # define transformation\n",
        "    spectrogram = T.Spectrogram(\n",
        "        n_fft=self.n_fft,\n",
        "        win_length=self.win_length,\n",
        "        hop_length=self.hop_length,\n",
        "        center=True,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=2.0,\n",
        "    )\n",
        "\n",
        "    # Perform transformation\n",
        "    self.modified_audio = audio\n",
        "    \n",
        "    \n",
        "    spec = spectrogram(audio.squeeze())\n",
        "    spec_DB = T.AmplitudeToDB()(spec)\n",
        "\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return spec_DB.expand((1, spec_DB.shape[0], spec_DB.shape[1])),label\n",
        "    #return spec_DB,label"
      ],
      "metadata": {
        "id": "kvLeM8qWy88s"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify Dataloaders\n",
        "import gc\n",
        "# Load train dataset\n",
        "X,y,CLASSES = utils.load(train_file)\n",
        "print(X.shape, y.shape, CLASSES.shape)\n",
        "# Train loader\n",
        "ds =  MiracleDataset(X,y, loader='train',transform= Compose( [\n",
        "        RandomApply([PolarityInversion()], p=0.8),\n",
        "        RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n",
        "        RandomApply([Gain()], p=0.2),\n",
        "        RandomApply([HighLowPass(sample_rate=22050)], p=0.8)\n",
        "    ]) )\n",
        "trainloader =  DataLoader(ds, batch_size= 64, shuffle=True, num_workers=0) \n",
        "del X\n",
        "del y\n",
        "gc.collect()\n",
        "##Load Validattion dataset\n",
        "X,y,CLASSES = utils.load(val_file)\n",
        "\n",
        "ds =  MiracleDataset(X,y)\n",
        "validationloader =  DataLoader(ds, batch_size= 64, shuffle=True, num_workers=0) \n",
        "del X\n",
        "del y\n",
        "gc.collect()\n",
        "# Load Test dataset\n",
        "X,y,CLASSES = utils.load(test_file)\n",
        "\n",
        "ds =  MiracleDataset(X,y)\n",
        "testloader =  DataLoader(ds, batch_size= 64, shuffle=True, num_workers=0)\n",
        "del X\n",
        "del y\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "W9HvmwV0zDpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec5a5662-5e49-4618-d603-76eff55a1819"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7994, 66150) (7994,) (11,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Validation and test loaders\n",
        "\n",
        "# samples,labels = next(iter(trainloader)) \n",
        "# print(samples.shape)\n",
        "\n",
        "#samples,labels = next(iter(testloader)) \n",
        "#print(samples.shape)\n",
        "\n",
        "# samples,labels = next(iter(validationloader)) \n",
        "# print(samples.shape)"
      ],
      "metadata": {
        "id": "zFX8UjYw2QFP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6IpLuh8frv-b"
      },
      "source": [
        "## Train a simple CNN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {},
        "id": "cdV6RuO8rv-d"
      },
      "outputs": [],
      "source": [
        "# Make a CNN & train it to predict genres.\n",
        "\n",
        "class music_net(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"Intitalize neural net layers\"\"\"\n",
        "    super(music_net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
        "    self.fc1 = nn.Linear(in_features=3584, out_features=10)\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
        "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
        "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
        "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Conv layer 1.\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 2.\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 3.\n",
        "    x = self.conv3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 4.\n",
        "    x = self.conv4(x)\n",
        "    x = self.batchnorm4(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 5.\n",
        "    x = self.conv5(x)\n",
        "    x = self.batchnorm5(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Fully connected layer 1.\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    x = F.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, validation_loader, epochs):\n",
        "  criterion =  nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "  train_loss, validation_loss = [], []\n",
        "  train_acc, validation_acc = [], []\n",
        "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
        "    tepochs.set_description('Training')\n",
        "    for epoch in tepochs:\n",
        "      model.train()\n",
        "      # keep track of the running loss\n",
        "      running_loss = 0.\n",
        "      correct, total = 0, 0\n",
        "\n",
        "      for data, target in train_loader:\n",
        "        # getting the training set\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Get the model output (call the model with the data from this batch)\n",
        "        output = model(data)\n",
        "        # Zero the gradients out)\n",
        "        optimizer.zero_grad()\n",
        "        # Get the Loss\n",
        "        loss  = criterion(output, target)\n",
        "        # Calculate the gradients\n",
        "        loss.backward()\n",
        "        # Update the weights (using the training step of the optimizer)\n",
        "        optimizer.step()\n",
        "\n",
        "        tepochs.set_postfix(loss=loss.item())\n",
        "        running_loss += loss  # add the loss for this batch\n",
        "\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "      # append the loss for this epoch\n",
        "      train_loss.append(running_loss/len(train_loader))\n",
        "      train_acc.append(correct/total)\n",
        "\n",
        "      # evaluate on validation data\n",
        "      model.eval()\n",
        "      running_loss = 0.\n",
        "      correct, total = 0, 0\n",
        "\n",
        "      for data, target in validation_loader:\n",
        "        # getting the validation set\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        tepochs.set_postfix(loss=loss.item())\n",
        "        running_loss += loss.item()\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "      validation_loss.append(running_loss/len(validation_loader))\n",
        "      validation_acc.append(correct/total)\n",
        "\n",
        "      if epoch%10 == 0:\n",
        "        model_dir = \"/content/drive/MyDrive/NMA/models\"\n",
        "        Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
        "        name = 'model' + tr(epoch) + 'ep.pt'\n",
        "\n",
        "        torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': net.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': train_loss,\n",
        "                    }, Path(model_dir, name))\n",
        "\n",
        "  return train_loss, train_acc, validation_loss, validation_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "print(device)"
      ],
      "metadata": {
        "id": "H7Fv04XQxljr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c3199b-9d39-40f8-ff37-7adefd59bbd7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 26 15:36:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "sample_path = '/content/drive/MyDrive/NMA/Data/train/reggae/reggae.00001.wav'\n",
        "audio, sr = torchaudio.load(sample_path)"
      ],
      "metadata": {
        "id": "Q5x1FS7t5gw7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "%prun\n",
        "\n",
        "spectrogram = T.Spectrogram(\n",
        "    n_fft=1024,\n",
        "    win_length=None,\n",
        "    hop_length=512,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        ")\n",
        "\n",
        "spec = spectrogram(audio)  \n",
        "spec_DB = T.AmplitudeToDB()(spec)"
      ],
      "metadata": {
        "id": "7Pvthvld4r5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a90893-062b-4379-ea3c-b39a86f52ed1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.96 µs\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "%prun\n",
        "\n",
        "\n",
        "transform= Compose( [\n",
        "        RandomApply([PolarityInversion()], p=0.8),\n",
        "        RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n",
        "        RandomApply([Gain()], p=0.2),\n",
        "        RandomApply([HighLowPass(sample_rate=22050)], p=0.8)\n",
        "    ])\n",
        "\n",
        "audio = transform(audio)"
      ],
      "metadata": {
        "id": "eouAn8oz6WGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff26555f-5b0b-487f-facf-92427144ff6f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
            "Wall time: 5.96 µs\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "%prun\n",
        "\n",
        "audio = PitchShift(sample_rate=sr, n_samples=audio.shape[0], pitch_shift_min=4, pitch_shift_max=5)(audio)"
      ],
      "metadata": {
        "id": "Vt6V8RWZ6yrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebd2fb4-b0e7-4120-d0ac-fda52f45b482"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {},
        "id": "1x59DP-Qrv-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474,
          "referenced_widgets": [
            "47742e772da74fdfbd2b6912143c5c5e",
            "daffa992bde743a6b486258be3d2b2f6",
            "db80faa37fe148a1b5dcb259f7019435",
            "d95ff9714b524582a4d0309497e6782a",
            "2b881caf6b2a4baf9e60dfba1e7b311e",
            "1c361b0833f24a83bdde12e058c94cc4",
            "fc105f67131e4c55ad70107e7c8611ab",
            "1d4003ed85ca424b92261cc170c5be05",
            "dee7c542123a42d08e3562c7f0fc3b0f",
            "424661b28e2847498ddc353149873e17",
            "664ae4dcc84d47e5b44189e96ac6db9f"
          ]
        },
        "outputId": "bfe3e9eb-a8bb-4554-dd98-4e33416b1b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47742e772da74fdfbd2b6912143c5c5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-a9f23202665b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prun'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmusic_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidationloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-24656f132ba1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, validation_loader, epochs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Calculate the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Update the weights (using the training step of the optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ],
      "source": [
        "# Run training.\n",
        "%prun\n",
        "net = music_net().to(device)\n",
        "train_loss, train_acc, validation_loss, validation_acc = train(net, device, trainloader, validationloader, 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss\n",
        "_train_loss, _train_acc, _validation_loss, _validation_acc = [], [], [], []\n",
        "for loss, acc, val_loss, val_acc in zip(train_loss, train_acc, validation_loss, validation_acc):\n",
        "  _train_loss.append(loss.to(\"cpu\").detach().numpy())\n",
        "  _train_acc.append(acc)\n",
        "  _validation_loss.append(val_loss)\n",
        "  _validation_acc.append(val_acc)\n",
        "plot_loss_accuracy(_train_loss, _train_acc, _validation_loss, _validation_acc)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'val_loss': _validation_loss, 'train_loss': _train_loss , 'train_acc': _train_acc, 'val_acc': _validation_acc})\n",
        "df.to_csv('/content/drive/MyDrive/NMA/Data/Stats/output_vanilla1_augment1.csv')"
      ],
      "metadata": {
        "id": "MputM8KDH-Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Loading models"
      ],
      "metadata": {
        "id": "jUdgRv0Tz2TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving\n",
        "model_dir = \"/content/drive/MyDrive/NMA/models\"\n",
        "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "torch.save({\n",
        "            'epoch': 20,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': train_loss,\n",
        "            }, Path(model_dir, \"model_augmented_20_vanilla1_augment1.pt\"))\n"
      ],
      "metadata": {
        "id": "bgBg0BYaya0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "\n",
        "def get_all_preds(model, loader, device):\n",
        "    model.eval()\n",
        "    all_preds = torch.tensor([]).to(device)\n",
        "    all_labels = torch.tensor([]).to(device)\n",
        "    for batch in loader:\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        preds = model(images)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "        all_labels = torch.cat(\n",
        "            (all_labels, labels)\n",
        "             ,dim=0)\n",
        "        \n",
        "    return all_preds, all_labels"
      ],
      "metadata": {
        "id": "9CIE6j-L2SK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading\n",
        "model_dir = \"/content/drive/MyDrive/NMA/models\"\n",
        "\n",
        "net = music_net().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
        "\n",
        "checkpoint = torch.load(Path(model_dir, \"model_augmented_20.pt\"))\n",
        "net.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "prob_class, true_labels = get_all_preds(net, testloader, device)"
      ],
      "metadata": {
        "id": "DHEYHlwYIEt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "print(prob_class.shape)\n",
        "pred_class = torch.argmax(prob_class, dim=1)\n",
        "print(pred_class.shape)\n",
        "\n",
        "print(true_labels.shape)\n",
        "\n",
        "conf_mat = metrics.confusion_matrix(true_labels.to('cpu').detach().numpy(), pred_class.to('cpu').detach().numpy())"
      ],
      "metadata": {
        "id": "6_fQKl8nzsFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat_norm = conf_mat / conf_mat.sum(axis=1)\n",
        "df_cm = pd.DataFrame(conf_mat_norm, index = [i for i in CLASSES],\n",
        "                  columns = [i for i in CLASSES])\n",
        "plt.figure(figsize = (10,7))\n",
        "\n",
        "CNF = sns.heatmap(df_cm, annot=True)\n",
        "fig = CNF.get_figure()\n",
        "fig.savefig(\"/content/drive/MyDrive/NMA/figures/CNF.png\") "
      ],
      "metadata": {
        "id": "XH_QBAud_vZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = metrics.classification_report(true_labels.to('cpu').detach().numpy(), pred_class.to('cpu').detach().numpy(),target_names=CLASSES)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "0wFrUt9X-Gur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hook the feature maps\n",
        "print(net)"
      ],
      "metadata": {
        "id": "DprXsix5ZYyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(net, (1, 513, 130))"
      ],
      "metadata": {
        "id": "Tw_RU564ZduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### HELPER FUNCTION FOR FEATURE EXTRACTION\n",
        "# Idea: https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/05/27/extracting-features.html\n",
        "# placeholder for batch features\n",
        "features = {}\n",
        "\n",
        "def get_features(name):\n",
        "    def hook(model, input, output):\n",
        "        features[name] = output.detach()\n",
        "    return hook"
      ],
      "metadata": {
        "id": "r3Xr39AraOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### REGISTER HOOK\n",
        "\n",
        "net.fc1.register_forward_hook(get_features('feats'))"
      ],
      "metadata": {
        "id": "dW8VyIjCaq6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### FEATURE EXTRACTION LOOP\n",
        "\n",
        "# placeholders\n",
        "PREDS = []\n",
        "FEATS = []\n",
        "LABELS = []\n",
        "\n",
        "\n",
        "# loop through batches\n",
        "for idx, batch in enumerate(testloader):\n",
        "    net.eval()\n",
        "    # move to device\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "       \n",
        "    # forward pass [with feature extraction]\n",
        "    preds = net(images)\n",
        "    \n",
        "    # add feats and preds to lists\n",
        "    PREDS.append(preds.detach().cpu().numpy())\n",
        "    FEATS.append(features['feats'].cpu().numpy())\n",
        "    LABELS.append(labels.cpu().numpy())\n",
        "    # # early stop\n",
        "    # if idx == 9:\n",
        "    #     break"
      ],
      "metadata": {
        "id": "QcrA3_14aryU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### INSPECT FEATURES\n",
        "\n",
        "PREDS = np.concatenate(PREDS)\n",
        "FEATS = np.concatenate(FEATS)\n",
        "LABELS = np.concatenate(LABELS)\n",
        "\n",
        "print('- preds shape:', PREDS.shape)\n",
        "print('- feats shape:', FEATS.shape)\n",
        "print('- labels shape:', LABELS.shape)"
      ],
      "metadata": {
        "id": "xKod27jrcPTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats_reshaped = FEATS.reshape(FEATS.shape[0], -1)\n",
        "feats_reshaped.shape\n"
      ],
      "metadata": {
        "id": "MYfftMFHcamW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(feats_reshaped)\n",
        "\n",
        "# sns.scatterplot(feat_data)"
      ],
      "metadata": {
        "id": "9aQEj04ac8ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(14, 10))\n",
        "\n",
        "new_labels = [CLASSES[i] for i in LABELS]\n",
        "\n",
        "feat_dict= {'X1': X_embedded[:, 0], 'X2': X_embedded[:, 1], 'LABELS': new_labels}\n",
        "\n",
        "feat_data = pd.DataFrame(data=feat_dict)\n",
        "\n",
        "scatterplot = sns.scatterplot(data=feat_data, x=\"X1\", y=\"X2\", hue=\"LABELS\", palette=\"deep\")\n",
        "fig = scatterplot.get_figure()\n",
        "fig.savefig(\"/content/drive/MyDrive/NMA/figures/Scatterplot02.png\") "
      ],
      "metadata": {
        "id": "GdJ1I2b6fNmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "import timm\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "iE9Zq5RulMye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading\n",
        "\n",
        "\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=10)\n",
        "model_dir = \"/content/drive/MyDrive/NMA/models/transformer\"\n",
        "\n",
        "checkpoint = torch.load(Path(model_dir, \"Fine_tune_spectrograms_21.params\"))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n"
      ],
      "metadata": {
        "id": "PX47v1TZk6sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "dh57xhXFk_Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify dataset class\n",
        "class MiracleDatasetForTransformer(Dataset):\n",
        "  def __init__(self, X:np.ndarray,y:np.ndarray,n_fft=1024,hop_length = 512,sr=22050, loader=None,win_length = None,transform=None, target_transform=None):\n",
        "    self.n_fft = n_fft\n",
        "    self.win_length = win_length\n",
        "    self.hop_length = hop_length\n",
        "    self.sr = sr\n",
        "    self.X =  X\n",
        "    self.labels =  y \n",
        "    self.transform =  transform \n",
        "    self.target_transform =  target_transform\n",
        "    self.loader = loader\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    audio =  torch.from_numpy(self.X[idx])\n",
        "    # audio.to(device)\n",
        "    #audio =  self.X[idx].to(device)\n",
        "    label =  self.labels[idx]\n",
        "    #label = torch.from_numpy(label)\n",
        "    #label.to(device)\n",
        "\n",
        "    if self.transform and self.loader == 'train':\n",
        "      self.orig_audio = audio\n",
        "      audio = self.transform(audio)\n",
        "      #Manual pitchshifting \n",
        "      p01 = torch.rand(1)\n",
        "      p02 = torch.rand(1)\n",
        "      if p01 <= 0.4:\n",
        "        audio = PitchShift(sample_rate=self.sr, n_samples=audio.shape[0], pitch_shift_min=4, pitch_shift_max=5)(audio.reshape(1,audio.shape[0]))\n",
        "        #print(f'Shape after Pitching: {audio.shape}')\n",
        "      # if p02 <= 0.3:\n",
        "      #   audio = Reverb(sample_rate=self.sr,reverberance_min=90,reverberance_max=91,room_size_min=90,room_size_max=91,)(audio)\n",
        "      #   print(f'Shape after Reverbing: {audio.shape}')\n",
        "    # define transformation\n",
        "    spectrogram = T.Spectrogram(\n",
        "        n_fft=self.n_fft,\n",
        "        win_length=self.win_length,\n",
        "        hop_length=self.hop_length,\n",
        "        center=True,\n",
        "        pad_mode=\"reflect\",\n",
        "        power=2.0,\n",
        "    )\n",
        "\n",
        "    # Perform transformation\n",
        "    self.modified_audio = audio\n",
        "    \n",
        "    \n",
        "    spec = spectrogram(audio.squeeze())\n",
        "    spec_DB = T.AmplitudeToDB()(spec)\n",
        "\n",
        "\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    resizer = transforms.Resize((224,224))\n",
        "    return resizer(spec_DB.expand((3, spec_DB.shape[0], spec_DB.shape[1]))),label\n",
        "    #return spec_DB,label"
      ],
      "metadata": {
        "id": "GC48VTW_qLcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y,CLASSES = utils.load(test_file)\n",
        "\n",
        "dst =  MiracleDatasetForTransformer(X,y)\n",
        "\n",
        "transformer_testloader =  DataLoader(dst, batch_size= 16, shuffle=True, num_workers=0) "
      ],
      "metadata": {
        "id": "dlrTQNUcqOZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X\n",
        "del y"
      ],
      "metadata": {
        "id": "cINsyhbIuFhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_features = {}\n",
        "\n",
        "def get_features(name):\n",
        "    def hook(model, input, output):\n",
        "        transformer_features[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "\n",
        "model.head.register_forward_hook(get_features('feats'))\n",
        "\n",
        "##### FEATURE EXTRACTION LOOP\n",
        "# placeholders\n",
        "transformer_PREDS = []\n",
        "transformer_FEATS = []\n",
        "transformer_LABELS = []\n",
        "\n",
        "# loop through batches\n",
        "for idx, batch in enumerate(transformer_testloader):\n",
        "    model.eval()\n",
        "    # move to device\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "\n",
        "    # forward pass [with feature extraction]\n",
        "    preds = model(images)\n",
        "    \n",
        "    # add feats and preds to lists\n",
        "    transformer_PREDS.append(preds.detach().cpu().numpy())\n",
        "    transformer_FEATS.append(transformer_features['feats'].cpu().numpy())\n",
        "    transformer_LABELS.append(labels.cpu().numpy())\n",
        "    # # early stop\n",
        "    # if idx == 9:\n",
        "    #     break\n",
        "\n",
        "transformer_PREDS = np.concatenate(transformer_PREDS)\n",
        "transformer_FEATS = np.concatenate(transformer_FEATS)\n",
        "transformer_LABELS = np.concatenate(transformer_LABELS)\n"
      ],
      "metadata": {
        "id": "ZTBzDUIOmnkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "##### INSPECT FEATURES\n",
        "\n",
        "\n",
        "\n",
        "print('- preds shape:', transformer_PREDS.shape)\n",
        "print('- feats shape:', transformer_FEATS.shape)\n",
        "print('- labels shape:', transformer_LABELS.shape)\n",
        "\n",
        "transformer_feats_reshaped = transformer_FEATS.reshape(transformer_FEATS.shape[0], -1)\n",
        "transformer_feats_reshaped.shape\n",
        "\n",
        "\n",
        "transformer_X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(transformer_feats_reshaped)\n",
        "\n"
      ],
      "metadata": {
        "id": "nA4_w5Tnw8Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(14, 10))\n",
        "\n",
        "new_labels = [CLASSES[i] for i in transformer_LABELS]\n",
        "\n",
        "transformer_feat_dict= {'X1': transformer_X_embedded[:, 0], 'X2': transformer_X_embedded[:, 1], 'LABELS': new_labels}\n",
        "\n",
        "feat_data = pd.DataFrame(data=transformer_feat_dict)\n",
        "\n",
        "scatterplot = sns.scatterplot(data=transformer_feat_dict, x=\"X1\", y=\"X2\", hue=\"LABELS\", palette=\"deep\")\n",
        "fig = scatterplot.get_figure()\n",
        "fig.savefig(\"/content/drive/MyDrive/NMA/figures/OptimusPrime64.png\") "
      ],
      "metadata": {
        "id": "bzHzJ0PfzegW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(1, 2,figsize=(18,8))\n",
        "sns.scatterplot(data=feat_data, x=\"X1\", y=\"X2\", hue=\"LABELS\", palette=\"deep\", ax=axes[0])\n",
        "sns.scatterplot(data=transformer_feat_dict, x=\"X1\", y=\"X2\", hue=\"LABELS\", palette=\"deep\", ax=axes[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tSO6mAKXoX6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_model_plus_eval_vanilla1_augment1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47742e772da74fdfbd2b6912143c5c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_daffa992bde743a6b486258be3d2b2f6",
              "IPY_MODEL_db80faa37fe148a1b5dcb259f7019435",
              "IPY_MODEL_d95ff9714b524582a4d0309497e6782a"
            ],
            "layout": "IPY_MODEL_2b881caf6b2a4baf9e60dfba1e7b311e"
          }
        },
        "daffa992bde743a6b486258be3d2b2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c361b0833f24a83bdde12e058c94cc4",
            "placeholder": "​",
            "style": "IPY_MODEL_fc105f67131e4c55ad70107e7c8611ab",
            "value": "Training:   0%"
          }
        },
        "db80faa37fe148a1b5dcb259f7019435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d4003ed85ca424b92261cc170c5be05",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dee7c542123a42d08e3562c7f0fc3b0f",
            "value": 0
          }
        },
        "d95ff9714b524582a4d0309497e6782a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_424661b28e2847498ddc353149873e17",
            "placeholder": "​",
            "style": "IPY_MODEL_664ae4dcc84d47e5b44189e96ac6db9f",
            "value": " 0/50 [00:09&lt;?, ?epoch/s]"
          }
        },
        "2b881caf6b2a4baf9e60dfba1e7b311e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c361b0833f24a83bdde12e058c94cc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc105f67131e4c55ad70107e7c8611ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d4003ed85ca424b92261cc170c5be05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee7c542123a42d08e3562c7f0fc3b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "424661b28e2847498ddc353149873e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664ae4dcc84d47e5b44189e96ac6db9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}